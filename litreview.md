# Wearable Foundation Models for Physiological Signals (2022–2025)

## Introduction  
Wearable devices such as smartwatches and fitness bands can continuously measure a variety of physiological and behavioral signals from users. There is **growing evidence** that these devices can promote healthy behaviors, aid in disease detection, and improve treatment monitoring ([Scaling wearable foundation models](https://research.google/blog/scaling-wearable-foundation-models/#:~:text=Wearable%20devices%20that%20measure%20physiological,outputs%20into%20more%20meaningful%20representations)). The data collected are *continuous, longitudinal,* and often *multimodal*, ranging from heart activity and motion to skin conductance and temperature. However, raw sensor streams are noisy and complex, making them hard for humans (and traditional algorithms) to interpret directly ([Scaling wearable foundation models](https://research.google/blog/scaling-wearable-foundation-models/#:~:text=amounts%20of%20continuous%2C%20longitudinal%2C%20and,outputs%20into%20more%20meaningful%20representations)). Traditionally, wearable analytics relied on supervised models trained for specific tasks (e.g. activity classification), but labeled events are sparse and imbalanced, leaving **vast amounts of unlabeled data** underutilized ([Scaling wearable foundation models](https://research.google/blog/scaling-wearable-foundation-models/#:~:text=Historically%2C%20algorithms%20for%20wearable%20sensors,Third%2C%20there)). This has spurred interest in **foundation models** for wearables – large models pretrained on unlabeled sensor data to learn general representations that can be adapted to many downstream health tasks.

In the past three years, researchers have drawn on advances in self-supervised learning (SSL) to develop wearable foundation models. SSL can leverage unlabeled data by training on proxy **pretext tasks** (e.g. predicting masked parts of a signal) that force models to learn meaningful features ([Scaling wearable foundation models](https://research.google/blog/scaling-wearable-foundation-models/#:~:text=Self,unlabeled%20data%20from%20wearable%20devices)). Inspired by the success of foundation models in NLP and computer vision, teams from both academia and industry (Google, Apple, Nokia Bell Labs, etc.) have begun creating analogous models for physiological time-series. These efforts must confront challenges unique to health signals: high noise levels ([
            SiamQuality: a ConvNet-based foundation model for photoplethysmography signals - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11334241/#:~:text=One%20reason%20that%20physiological%20measurement,of%20human%20physiology%20and%20the)), significant inter-person variability ([Subject-Aware Contrastive Learning for Biosignals - Apple Machine Learning Research](https://machinelearning.apple.com/research/subject-aware-contrastive-learning#:~:text=approach%20based%20on%20contrastive%20learning,EEG%20decoding%20and%20ECG%20anomaly)), and limited labeled outcomes for fine-tuning. The following sections review the state of wearable foundation models since ~2022, focusing on the **pretraining objectives** they employ – from contrastive learning and masked modeling to predictive and hybrid approaches – and how these models support downstream applications like biomarker discovery, anomaly detection, and digital phenotyping. We also discuss current gaps and future opportunities for making these models more generalizable, efficient, and privacy-preserving.

## Wearable Data and Modalities  
 ([Scaling wearable foundation models](https://research.google/blog/scaling-wearable-foundation-models/)) *Wearable devices (like a smartwatch) collect multiple physiological signals such as skin conductance (electrodermal activity), heart rate (e.g. from PPG), heart rate variability, motion (accelerometer), and altitude (barometer) continuously. Converting these raw signals into meaningful insights requires robust modeling of their temporal patterns ([Scaling wearable foundation models](https://research.google/blog/scaling-wearable-foundation-models/#:~:text=Wearable%20devices%20that%20measure%20physiological,outputs%20into%20more%20meaningful%20representations)) ([Scaling wearable foundation models](https://research.google/blog/scaling-wearable-foundation-models/#:~:text=amounts%20of%20continuous%2C%20longitudinal%2C%20and,outputs%20into%20more%20meaningful%20representations)).* 

Modern wearables can capture an array of biosignals: **photoplethysmography (PPG)** for heart rate and blood oxygen, **electrocardiogram (ECG)** for cardiac electrical activity, **accelerometry** for motion/activity, **gyroscope** for orientation, **electrodermal activity (EDA)** or skin conductance, **skin temperature**, and more. These signals are often sampled at varying rates and have very different characteristics (e.g. PPG is a pulsatile waveform, accelerometer data are tri-axial movement readings) ([[2412.09758] Toward Foundation Model for Multivariate Wearable Sensing of Physiological Signals](https://ar5iv.org/html/2412.09758v1#:~:text=Mobile%20and%20wearable%20sensors%20have,2024a)) ([[2412.09758] Toward Foundation Model for Multivariate Wearable Sensing of Physiological Signals](https://ar5iv.org/html/2412.09758v1#:~:text=Recently%2C%20various%20foundation%20models%20on,12)). They also exhibit multi-scale temporal dynamics – from millisecond-scale variations (heartbeats) to circadian patterns (sleep-wake cycles). Continuous monitoring over months yields **longitudinal datasets** that are both large and rich in temporal context. For example, Google’s recent study assembled a dataset of over *40 million hours* of wearable sensor data (heart rate, variability, EDA, motion, etc.) from 165,000 participants ([[2410.13638] Scaling Wearable Foundation Models](https://ar5iv.org/abs/2410.13638#:~:text=we%20investigate%20the%20scaling%20properties,downstream%20learning%20for%20tasks%20like)). Similarly, Apple’s Heart and Movement Study collected multi-year PPG and ECG signals from ~141,000 Apple Watch users ([Large-scale Training of Foundation Models for Wearable Biosignals - Apple Machine Learning Research](https://machinelearning.apple.com/research/large-scale-training#:~:text=consent%20from%20the%20large%20longitudinal,To%20the)). Such scale underscores the potential for foundation models – there is far more data than we have labels or specific hypotheses for, inviting automated representation learning.

Making sense of raw wearable signals is challenging due to noise and artifacts. Motion artifacts, sensor displacement, and environmental interference can corrupt PPG/ECG readings ([
            SiamQuality: a ConvNet-based foundation model for photoplethysmography signals - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11334241/#:~:text=One%20reason%20that%20physiological%20measurement,of%20human%20physiology%20and%20the)). There is also significant **person-specific structure** (each individual’s “baseline” heart rhythm or gait pattern differs), which can confound general models ([Subject-Aware Contrastive Learning for Biosignals - Apple Machine Learning Research](https://machinelearning.apple.com/research/subject-aware-contrastive-learning#:~:text=approach%20based%20on%20contrastive%20learning,EEG%20decoding%20and%20ECG%20anomaly)). The goal of wearable foundation models is to learn latent representations that capture the *important physiological patterns* (e.g. heart rhythm dynamics, activity signatures) while being robust to noise and subject variability. These representations can then be used for various downstream tasks – from detecting anomalies like arrhythmias to inferring daily behaviors – often with just a small amount of fine-tuning data. In the following, we survey the **pretraining strategies** that have been explored to train such foundation models on wearable data.

## Pretraining Objectives in Wearable Foundation Models  

A key differentiator among wearable foundation models is the **self-supervised objective** used during pretraining. Researchers have experimented with numerous objectives to coax models into learning useful features from unlabeled sensor streams. We organize the approaches into several categories: **contrastive learning**, **masked signal modeling**, **predictive (generative) modeling**, and **hybrid or specialized tasks**. Often, a single model may incorporate multiple objectives. Below, we catalog each category with examples from recent literature.

### Contrastive Learning on Wearable Signals  
Contrastive learning has become a popular approach for wearable data, inspired by its success in computer vision and audio. The core idea is to define pairs of “related” signal segments (positives) that the model should produce similar embeddings for, while pushing apart embeddings of unrelated segments (negatives). This teaches the model an invariant representation of the underlying physiological state, regardless of superficial differences in how the signal appears.

One prominent example is Apple’s foundation model for PPG and ECG signals ([Large-scale Training of Foundation Models for Wearable Biosignals](https://arxiv.org/html/2312.05409v2#:~:text=supervised%20learning%20using%20the%20unlabeled,trained%20foundation%20models%20readily%20encode)). Abbaspourazad *et al.* (2024) use a **momentum contrastive framework**: they select **positive pairs at the participant level**, meaning two different time segments from the *same user* are treated as similar ([Large-scale Training of Foundation Models for Wearable Biosignals - Apple Machine Learning Research](https://machinelearning.apple.com/research/large-scale-training#:~:text=Our%20self,ECG%20foundation%20models%20can%20enhance)). This leverages the intuition that readings from one person (even at different times) share person-specific physiological patterns. To implement this, they apply strong stochastic **augmentations** to each segment (such as adding noise, scaling, time-warping) and use a momentum encoder (à la MoCo) to generate embeddings. A **regularized InfoNCE loss** then brings embeddings from the same user together and pushes different users’ embeddings apart ([Large-scale Training of Foundation Models for Wearable Biosignals - Apple Machine Learning Research](https://machinelearning.apple.com/research/large-scale-training#:~:text=Our%20self,ECG%20foundation%20models%20can%20enhance)). This subject-aware contrastive strategy forces the model to encode traits that consistently characterize an individual’s signals, while ignoring transient noise. The authors found that such pretraining on **over 100k users** yielded embeddings that “readily encode information” about health conditions and demographics ([Large-scale Training of Foundation Models for Wearable Biosignals - Apple Machine Learning Research](https://machinelearning.apple.com/research/large-scale-training#:~:text=Our%20self,ECG%20foundation%20models%20can%20enhance)), even before fine-tuning. Notably, they compared contrastive vs. non-contrastive Siamese approaches and observed that **contrastive learning produced more informative embeddings** for both PPG and ECG ([Large-scale Training of Foundation Models for Wearable Biosignals](https://arxiv.org/html/2312.05409v2#:~:text=match%20at%20L503%20sex%20and,We%20observed%20that%20for%20both)).

Another contrastive approach addresses signal quality issues. Zhao *et al.* (2023) introduced **SiamQuality**, a foundation model to handle low-quality PPG data ([
            SiamQuality: a ConvNet-based foundation model for photoplethysmography signals - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11334241/#:~:text=Objective,36%20million%2030%20s%20PPG)). SiamQuality is built on a **SimSiam** architecture (stop-gradient Siamese networks) and learns to generate similar representations for two views of the *same physiological moment* – one high-quality PPG segment and one degraded by noise ([
            SiamQuality: a ConvNet-based foundation model for photoplethysmography signals - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11334241/#:~:text=signals,alignment%20of%20PPG%20representations%20more)). In effect, the model learns to be invariant to motion artifacts and dropouts: a clean PPG and a noisy PPG from the same time are “positives” that must align in feature space. This approach achieved significant improvements on downstream tasks like respiration rate estimation and atrial fibrillation detection under real-world noise, exceeding prior state-of-the-art by 75% in some cases ([
            SiamQuality: a ConvNet-based foundation model for photoplethysmography signals - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11334241/#:~:text=hospitalized%20intensive%20care%20patients%2C%20comprised,robust%20backbone%20for%20foundation%20models)). SiamQuality shows how contrastive objectives can be adapted (in this case, pairing clean/noisy signals) to learn *robust representations* that discount irrelevant signal perturbations.

A general form of contrastive SSL for time series is to define positives as *different augmented views of the same underlying sequence*. Many earlier works (TS2Vec, SimCLR adaptations) followed that paradigm. However, **participant-aware contrastive learning** has gained traction for wearables. Nokia Bell Labs’ **PaPaGei** model (2024) explicitly compares a participant-aware variant to vanilla contrastive learning ([PaPaGei: Open Foundation Models for Optical Physiological Signals](https://arxiv.org/html/2410.20542v1#:~:text=In%20PaPaGei,or%20evaluate%20on%20public%20datasets)) ([PaPaGei: Open Foundation Models for Optical Physiological Signals](https://arxiv.org/html/2410.20542v1#:~:text=obtain%20embeddings%20denoted%20,of%20randomly%20sampled%20PPG%20segments)). In PaPaGei’s *participant-aware objective* (dubbed *PaPaGei-P*), any two distinct PPG segments from the **same subject** are taken as a positive pair ([PaPaGei: Open Foundation Models for Optical Physiological Signals](https://arxiv.org/html/2410.20542v1#:~:text=Training,of%20embeddings%20from%20distinct%20subjects)). After augmenting each with random crops, noise, flips, etc., the model optimizes an InfoNCE loss (NT-Xent) that maximizes agreement between embeddings from the same person and differentiates embeddings from different people ([PaPaGei: Open Foundation Models for Optical Physiological Signals](https://arxiv.org/html/2410.20542v1#:~:text=Additionally%2C%20each%20augmentation%20includes%20hyper,of%20randomly%20sampled%20PPG%20segments)). By contrast, *“vanilla SimCLR”* on this data would have picked two random augmented views of the *same segment* as the positive pair ([PaPaGei: Open Foundation Models for Optical Physiological Signals](https://arxiv.org/html/2410.20542v1#:~:text=obtain%20embeddings%20denoted%20,of%20randomly%20sampled%20PPG%20segments)). The subject-level approach encourages learning person-specific features such as vascular characteristics or sensor placement idiosyncrasies. Interestingly, PaPaGei’s authors note that Apple’s approach was similar in spirit but was not evaluated on public datasets ([PaPaGei: Open Foundation Models for Optical Physiological Signals](https://arxiv.org/html/2410.20542v1#:~:text=In%20PaPaGei,or%20evaluate%20on%20public%20datasets)). The participant-aware pretraining helped PaPaGei achieve strong generalization across datasets – a sign that capturing subject invariances (and hence knowing what to ignore) can improve downstream robustness.

Beyond positive pair selection, some works integrate **adversarial objectives** to remove subject identity from representations. An earlier study by Cheng *et al.* (2021) proposed **Subject-Aware Contrastive Learning** with an adversarial network to enforce that the learned embedding cannot predict which subject it came from ([Subject-Aware Contrastive Learning for Biosignals - Apple Machine Learning Research](https://machinelearning.apple.com/research/subject-aware-contrastive-learning#:~:text=approach%20based%20on%20contrastive%20learning,EEG%20decoding%20and%20ECG%20anomaly)) ([Subject-Aware Contrastive Learning for Biosignals - Apple Machine Learning Research](https://machinelearning.apple.com/research/subject-aware-contrastive-learning#:~:text=specific%20contrastive%20loss%2C%20and%20,tuning%20with)). The idea is to combat inter-subject variability by *explicitly* making the representation subject-invariant, even as contrastive learning uses subject signals for positives. This is a reminder that depending on the goal, one might either want to **retain** subject-specific info (for personalization) or **discard** it (for generalization). Most foundation models aim for general features, but in health, personalization is also valuable. Balancing these needs is an open research question; contrastive frameworks are being adapted on both ends (some encouraging subject-specific clustering, others doing adversarial de-identification).

In summary, **contrastive learning** has proven effective for wearable foundation models in various forms: from classic augment-and-contrast (as in SimCLR/SimSiam), to pairing signals across time, quality, or individuals. These methods yield representations that are **invariant to noise** and certain transformations, and in some cases capture persistent subject factors. The result is often improved performance on tasks like event detection or vital sign estimation, with far fewer labels than a fully supervised approach would need ([
            SiamQuality: a ConvNet-based foundation model for photoplethysmography signals - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11334241/#:~:text=hospitalized%20intensive%20care%20patients%2C%20comprised,robust%20backbone%20for%20foundation%20models)). Contrastive objectives have been especially popular for **cardiorespiratory signals** (PPG, ECG) where domain-specific augmentations and pairing strategies (heartbeat shuffling, frequency filtering, etc.) can be applied to good effect.

### Masked Signal Modeling and Reconstruction  
Another major class of pretraining objectives treats wearable time-series data in analogy to masked language modeling (BERT) or masked image modeling. Here, the model is fed an input sequence with random portions **masked or removed**, and it must **reconstruct the missing values**. By learning to predict missing sensor readings, the model develops an understanding of the temporal dynamics and correlations in the data. This approach falls under **generative self-supervision**, since the model is trained to model (and regenerate) the original signal distribution.

Google’s **Large Sensor Model (LSM)** is a prominent example. LSM was trained on the enormous 40 million hours dataset mentioned earlier with a **random masking pretext task** ([[2410.13638] Scaling Wearable Foundation Models](https://ar5iv.org/abs/2410.13638#:~:text=people,on%20the%20random%20imputation%20task)). Specifically, they applied an extremely high masking ratio (80% of the input time-series patches were masked out) and tasked a transformer encoder to reconstruct the missing portions ([[2410.13638] Scaling Wearable Foundation Models](https://ar5iv.org/abs/2410.13638#:~:text=All%20pretraining%20experiments%20use%20an,be%20found%20in%20Appendix%20C)). The loss was simply the mean squared error between the reconstructed and true values for those masked segments ([[2410.13638] Scaling Wearable Foundation Models](https://ar5iv.org/abs/2410.13638#:~:text=optimize%20the%20masked%20signal%20reconstruction,across%20all%20the%20normalized%20signals)). By optimizing this **masked reconstruction loss**, the model effectively learns to fill in gaps in multimodal sensor data. This yields a powerful general model that can perform **imputation, interpolation, and extrapolation** across time and across sensors ([[2410.13638] Scaling Wearable Foundation Models](https://ar5iv.org/abs/2410.13638#:~:text=people,on%20the%20random%20imputation%20task)). In fact, LSM demonstrated up to *38% lower error* on imputing missing wearable data compared to traditional methods ([Scaling wearable foundation models](https://research.google/blog/scaling-wearable-foundation-models/#:~:text=In%20%E2%80%9CScaling%20Wearable%20Foundation%20Models%E2%80%9D%2C,over)) ([Scaling wearable foundation models](https://research.google/blog/scaling-wearable-foundation-models/#:~:text=present%20the%20results%20of%20our,over%20traditional%20imputation%20methods)). Beyond imputation, the learned representations transfer well: after such pretraining, only a small amount of fine-tuning data was needed to recognize activities like exercise from sensor streams ([[2410.13638] Scaling Wearable Foundation Models](https://ar5iv.org/abs/2410.13638#:~:text=create%20LSM%2C%20a%20multimodal%20foundation,like%20exercise%20and%20activity%20recognition)). The success of LSM answers a key question posed by its authors: *do scaling laws apply to wearable data?* The findings suggest yes – much like in language or vision, significantly better performance emerges by scaling up model size and data under a suitable self-supervised objective ([Scaling wearable foundation models](https://research.google/blog/scaling-wearable-foundation-models/#:~:text=neural%20models%20indicates%20that%20model,across%20diverse%20tasks%20and%20datasets)) ([Scaling wearable foundation models](https://research.google/blog/scaling-wearable-foundation-models/#:~:text=In%20%E2%80%9CScaling%20Wearable%20Foundation%20Models%E2%80%9D%2C,and%20model%20with%20respect%20to)) (in this case, masked modeling).

Other groups have also adopted masked signal modeling. **NormWear** (Luo *et al.* 2024), a foundation model spanning PPG, ECG, EEG, GSR (galvanic skin) and motion data, uses an **Masked Autoencoder (MAE)**-inspired strategy in pretraining ([[2412.09758] Toward Foundation Model for Multivariate Wearable Sensing of Physiological Signals](https://ar5iv.org/html/2412.09758v1#:~:text=Image%3A%20Refer%20to%20caption%20Figure,Overview%20of%20the%20pretrain%20pipeline)). After a clever tokenization of multi-modal signals into a unified sequence, NormWear masks random patches and trains the model to reconstruct them, following the approach of He *et al.* (2022) for images ([[2412.09758] Toward Foundation Model for Multivariate Wearable Sensing of Physiological Signals](https://ar5iv.org/html/2412.09758v1#:~:text=Image%3A%20Refer%20to%20caption%20Figure,Overview%20of%20the%20pretrain%20pipeline)). This helps the model learn a common latent space for very heterogeneous biosignals. Masked modeling is appealing for multimodal data because the model must capture cross-modal relationships to do a good job – e.g. if heart rate goes up (from PPG) and steps increase (from accelerometer), a masked model could infer one from the other. Indeed, a recent study from Apple on **multimodal physiological data** found that a joint masked autoencoder with cross-modal *dropout* leads to better integration of modalities than separate encoders ([Promoting Cross-Modal Representations to Improve Multimodal Foundation Models for Physiological Signals - Apple Machine Learning Research](https://machinelearning.apple.com/research/modal-representations#:~:text=costly%2C%20there%20is%20a%20lot,We%20also%20find%20that%20late)) ([Promoting Cross-Modal Representations to Improve Multimodal Foundation Models for Physiological Signals - Apple Machine Learning Research](https://machinelearning.apple.com/research/modal-representations#:~:text=successful%20multimodal%20training%2C%20as%20they,health%20data%2C%20even%20across%20diverse)). By randomly dropping out entire modalities during training, the model is forced to reconstruct missing channels using information from the remaining ones, thereby learning to fuse modalities and not rely on just one ([Promoting Cross-Modal Representations to Improve Multimodal Foundation Models for Physiological Signals - Apple Machine Learning Research](https://machinelearning.apple.com/research/modal-representations#:~:text=challenges%20in%20the%20PhysioNet%202018,analyze%20the%20model%27s%20representations%2C%20showing)) ([Promoting Cross-Modal Representations to Improve Multimodal Foundation Models for Physiological Signals - Apple Machine Learning Research](https://machinelearning.apple.com/research/modal-representations#:~:text=successful%20multimodal%20training%2C%20as%20they,health%20data%2C%20even%20across%20diverse)). They observed that this approach caused the transformer's attention maps to become more **cross-modal**, aligning temporally across signals, whereas contrastive late-fusion baselines were less effective in multi-task evaluations ([Promoting Cross-Modal Representations to Improve Multimodal Foundation Models for Physiological Signals - Apple Machine Learning Research](https://machinelearning.apple.com/research/modal-representations#:~:text=that%20can%20be%20linearly%20probed,also%20become%20more%20distributed%20in)) ([Promoting Cross-Modal Representations to Improve Multimodal Foundation Models for Physiological Signals - Apple Machine Learning Research](https://machinelearning.apple.com/research/modal-representations#:~:text=successful%20multimodal%20training%2C%20as%20they,health%20data%2C%20even%20across%20diverse)).

There have also been architectural enhancements to masked modeling for biosignals. One example is Apple’s **Frequency-Aware Masked Autoencoder (FAME)** proposed in 2024. In this model, the encoder includes specialized frequency-domain filters to better capture both local and global patterns in time-series ([Frequency-Aware Masked Autoencoders for Multimodal Pretraining on Biosignals - Apple Machine Learning Research](https://machinelearning.apple.com/research/frequency-aware-masked-autoencoders#:~:text=for%20biosignals%20that%20can%20be,We%20demonstrate%20the)) ([Frequency-Aware Masked Autoencoders for Multimodal Pretraining on Biosignals - Apple Machine Learning Research](https://machinelearning.apple.com/research/frequency-aware-masked-autoencoders#:~:text=with%20three%20key%20features%3A%20,outperform%20in%20transfer%20learning%20tasks)). In addition, it uses a **channel-independent** design where the same encoder weights are shared across different channels/sensors (with modality-specific spectral filters to account for differences) ([Frequency-Aware Masked Autoencoders for Multimodal Pretraining on Biosignals - Apple Machine Learning Research](https://machinelearning.apple.com/research/frequency-aware-masked-autoencoders#:~:text=with%20three%20key%20features%3A%20,outperform%20in%20transfer%20learning%20tasks)) ([Frequency-Aware Masked Autoencoders for Multimodal Pretraining on Biosignals - Apple Machine Learning Research](https://machinelearning.apple.com/research/frequency-aware-masked-autoencoders#:~:text=filters%20in%20the%20frequency%20space,outperform%20in%20transfer%20learning%20tasks)). This ensures that each sensor modality (ECG vs PPG vs accelerometer) is processed in a way that leverages common structure but also retains some uniqueness. Finally, a modality-combining transformer layer merges information across all channels ([Frequency-Aware Masked Autoencoders for Multimodal Pretraining on Biosignals - Apple Machine Learning Research](https://machinelearning.apple.com/research/frequency-aware-masked-autoencoders#:~:text=with%20three%20key%20features%3A%20,outperform%20in%20transfer%20learning%20tasks)) ([Frequency-Aware Masked Autoencoders for Multimodal Pretraining on Biosignals - Apple Machine Learning Research](https://machinelearning.apple.com/research/frequency-aware-masked-autoencoders#:~:text=filters%20in%20the%20frequency%20space,outperform%20in%20transfer%20learning%20tasks)). This architecture, paired with a masked reconstruction objective on multiple datasets, yielded a model that outperformed single-modality models in various transfer learning tasks ([Frequency-Aware Masked Autoencoders for Multimodal Pretraining on Biosignals - Apple Machine Learning Research](https://machinelearning.apple.com/research/frequency-aware-masked-autoencoders#:~:text=modality,outperform%20in%20transfer%20learning%20tasks)). The frequency-aware design acknowledges that physiological signals often have characteristic frequency bands (for instance, heart rate variability components, or motion signal spectra) – by incorporating this knowledge into the model, it can reconstruct signals more effectively and learn features that traditional time-domain only models might miss.

In summary, **masked signal modeling** has emerged as a powerful pretraining approach for wearable foundation models. It treats unlabeled time series as a puzzle to be solved: by hiding parts of the sequence, the model must *learn the dynamics* to fill in the blanks. The result is a form of **self-supervised forecasting** of the immediate context, which naturally trains the model on temporal patterns, seasonality, and cross-sensor correlations. Empirically, this has led to strong performance on tasks requiring *generative understanding* (like data imputation) and has proven beneficial for downstream classification/regression tasks as well ([[2410.13638] Scaling Wearable Foundation Models](https://ar5iv.org/abs/2410.13638#:~:text=create%20LSM%2C%20a%20multimodal%20foundation,like%20exercise%20and%20activity%20recognition)) ([Promoting Cross-Modal Representations to Improve Multimodal Foundation Models for Physiological Signals - Apple Machine Learning Research](https://machinelearning.apple.com/research/modal-representations#:~:text=challenges%20in%20the%20PhysioNet%202018,analyze%20the%20model%27s%20representations%2C%20showing)). Importantly, masked modeling can be effectively applied to **multimodal** wearable data, especially when combined with strategies like modality dropout to ensure the model truly learns to integrate information across channels ([Promoting Cross-Modal Representations to Improve Multimodal Foundation Models for Physiological Signals - Apple Machine Learning Research](https://machinelearning.apple.com/research/modal-representations#:~:text=costly%2C%20there%20is%20a%20lot,We%20also%20find%20that%20late)) ([Promoting Cross-Modal Representations to Improve Multimodal Foundation Models for Physiological Signals - Apple Machine Learning Research](https://machinelearning.apple.com/research/modal-representations#:~:text=successful%20multimodal%20training%2C%20as%20they,health%20data%2C%20even%20across%20diverse)). As wearable datasets continue to grow, masked modeling provides a straightforward way to leverage scale: models like LSM show that feeding *massive amounts of unlabeled sensor data* into an appropriately scaled autoencoder leads to increasingly powerful representations ([Scaling wearable foundation models](https://research.google/blog/scaling-wearable-foundation-models/#:~:text=neural%20models%20indicates%20that%20model,across%20diverse%20tasks%20and%20datasets)) ([Scaling wearable foundation models](https://research.google/blog/scaling-wearable-foundation-models/#:~:text=In%20%E2%80%9CScaling%20Wearable%20Foundation%20Models%E2%80%9D%2C,and%20model%20with%20respect%20to)).

### Predictive and Generative Objectives  
Some foundation models take a more explicitly **predictive (or generative) approach** to pretraining, akin to treating the time series as a language to be modeled or a sequence to be forecasted. Instead of masking random chunks, these methods often train models to predict the *next* data point(s) in sequence or entire future segments, given the past. This category includes autoregressive language-model style training on discretized signals, as well as frameworks like contrastive predictive coding that aim to model temporal evolution.

A notable example is **Chronos** (Ansari *et al.* 2024), which adapts large language model techniques to time-series forecasting ([The Rise of Foundation Models in Time Series - Videns](https://www.videns.ai/en-ca/blog/lessor-des-modeles-fondamentaux-dans-les-series-temporelles-un-changement-de-paradigme-ou-juste-un-autre-engouement#:~:text=The%20Rise%20of%20Foundation%20Models,The%20Chronos)) ([[2403.07815] Chronos: Learning the Language of Time Series](https://arxiv.org/abs/2403.07815#:~:text=,classical%20local%20models%20and%20deep)). Chronos tokenizes continuous time series values using quantization (converting real-valued sensor readings into a discrete vocabulary of tokens) and then trains a Transformer (based on the T5 architecture) to predict the sequence of tokens via a standard language modeling objective (minimizing cross-entropy loss) ([[2403.07815] Chronos: Learning the Language of Time Series](https://arxiv.org/abs/2403.07815#:~:text=,a%29%20significantly%20outperform)). In essence, it **“learns the language of time series”** by treating a multivariate sequence as if it were a sentence to complete. Chronos was pretrained on a large corpus of 42 public datasets plus additional synthetic time series (generated via Gaussian process simulations) to expose the model to diverse patterns ([[2403.07815] Chronos: Learning the Language of Time Series](https://arxiv.org/abs/2403.07815#:~:text=scaling%20and%20quantization%20into%20a,b%29%20have)) ([[2403.07815] Chronos: Learning the Language of Time Series](https://arxiv.org/abs/2403.07815#:~:text=via%20the%20cross,Our%20results)). The resulting models, ranging from 20M to 710M parameters, demonstrated strong performance in zero-shot forecasting: on new datasets not seen during training, Chronos often matched or exceeded specialized models that were trained with supervision on those datasets ([[2403.07815] Chronos: Learning the Language of Time Series](https://arxiv.org/abs/2403.07815#:~:text=consisting%20of%2042%20datasets%2C%20and,to%20greatly%20simplify%20forecasting%20pipelines)). This highlights the promise of **decoder-only, autoregressive pretraining** for time series – much like GPT-3 did for text, Chronos shows a single pretrained model can generalize to forecasting tasks across many domains. While Chronos’ focus was broad (including economic and weather data), the approach could be applied to wearable data as well. Indeed, any long, continuous physiological signal can be discretized and fed into such a model, though tokenization may sacrifice some precision unless done cleverly.

Another recent model from Google, **TimesFM** (Das *et al.* 2023), follows a similar philosophy for universal forecasting. It uses a *decoder-only Transformer* trained on over **100 billion real-world time points** from a variety of domains ([A decoder-only foundation model for time-series forecasting](https://research.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting/#:~:text=%E2%80%9CA%20decoder,our%20HuggingFace%20and%20GitHub%20repos)) ([A decoder-only foundation model for time-series forecasting](https://research.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting/#:~:text=TimesFM%20is%20a%20forecasting%20model%2C,from%20different%20domains%20and%20granularities)). The training objective is to forecast future values given the past context – essentially learning a function to generate the continuation of the time series. TimesFM achieved impressive zero-shot forecasting results on benchmarks from retail demand to traffic and healthcare, indicating that a sufficiently large model can capture generic time-series patterns ([A decoder-only foundation model for time-series forecasting](https://research.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting/#:~:text=%E2%80%9CA%20decoder,our%20HuggingFace%20and%20GitHub%20repos)). An interesting aspect is that TimesFM and Chronos were both able to perform *zero-shot inference*, meaning they could take a completely new time series and produce reasonable forecasts without any fine-tuning ([A decoder-only foundation model for time-series forecasting](https://research.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting/#:~:text=%E2%80%9CA%20decoder,our%20HuggingFace%20and%20GitHub%20repos)) ([[2403.07815] Chronos: Learning the Language of Time Series](https://arxiv.org/abs/2403.07815#:~:text=consisting%20of%2042%20datasets%2C%20and,to%20greatly%20simplify%20forecasting%20pipelines)). In wearable applications, this could translate to models that, for example, can start predicting a user’s sleep patterns or heart rate trends immediately upon deployment by leveraging their pre-learned knowledge of human circadian rhythms and physiology.

Beyond language-model style approaches, other predictive self-supervised tasks have been explored. **Contrastive Predictive Coding (CPC)**, originally developed for audio, has been applied to biomedical signals as well. CPC trains an encoder to summarize the recent past of a signal and then uses an InfoNCE loss to ensure that this summary can predict the *true future* segment as opposed to negative (random) future segments. This way, the model learns a latent space where one can perform “next-step prediction” implicitly. While CPC itself is slightly older than our 3-year window, variants of it influence modern designs. For instance, some models include a **temporal ordering task** – e.g., shuffling segments of a time series and asking the model to identify the correct order (a “jigsaw puzzle” for time). Such tasks compel the model to learn causality and progression in time. We also see **forecasting as an auxiliary task**: a foundation model may be trained not just to reconstruct missing data (autoencoding) but also to extrapolate into the future by a certain horizon. If a wearable dataset has inherent forecasting tasks (like next day’s heart rate variability), including that in pretraining can be beneficial.

It’s worth noting that generative objectives (like next-token prediction) and masked modeling objectives are closely related – both are about predicting withheld information, just arranged differently. Masking typically provides *bidirectional* context (like BERT: the model sees both sides of a gap), whereas autoregressive prediction is strictly *causal* (predicting forward without seeing future points). Some time-series foundation models combine the two: for example, a model might mask a window in the middle of a sequence (requiring bidirectional interpolation) or at the end of a sequence (requiring extrapolation). By varying the position of the mask, one can train a model to handle both interpolation and forecasting.

In summary, **self-supervised temporal prediction** objectives treat wearable data as something to be *predicted or generated*. Approaches like Chronos show the extreme end of this spectrum – turning signals into a “language” and training enormous models to generate them. These methods have yielded models with strong **zero-shot forecasting** ability, which could be very useful for wearable applications like predicting health trajectories (e.g., forecasting stress levels or cardiac metrics days in advance) without task-specific training. While pure generative pretraining is computationally heavy, it provides perhaps the most **holistic** learning of a data distribution. As foundation models for wearables evolve, we may see increased use of such objectives, potentially combined with domain-specific tweaks (e.g., tokenizing physiological ranges in a smart way, or predicting not just raw signals but higher-level trends).

### Hybrid and Specialized Pretraining Approaches  
Many wearable foundation models do not rely on a single objective, but rather **combine multiple self-supervised tasks** or introduce specialized objectives tuned to physiological data. These hybrid approaches aim to capture different aspects of the data simultaneously (e.g., global trends vs. local morphology), or to align the learned representations with external information (like semantic labels or other modalities). We highlight a few influential examples:

**NormWear’s multi-stage pretraining & alignment:** After using a masked reconstruction backbone, NormWear introduced a novel *representation alignment* step ([[2412.09758] Toward Foundation Model for Multivariate Wearable Sensing of Physiological Signals](https://ar5iv.org/html/2412.09758v1#:~:text=pretraining%20on%20the%20pretraining%20datasets,estimation%20of%20the%20representation%20transformation)). In this stage, the model’s physiological embeddings were aligned to a **semantic space** by leveraging textual descriptions of the data. Concretely, Luo *et al.* generated multiple text sentences (descriptions or labels) for certain training signals and trained the model to **match signal embeddings with the embedding of the correct text** (using a pretrained language model to embed the text) ([[2412.09758] Toward Foundation Model for Multivariate Wearable Sensing of Physiological Signals](https://ar5iv.org/html/2412.09758v1#:~:text=pretraining%20on%20the%20pretraining%20datasets,estimation%20of%20the%20representation%20transformation)) ([[2412.09758] Toward Foundation Model for Multivariate Wearable Sensing of Physiological Signals](https://ar5iv.org/html/2412.09758v1#:~:text=We%20achieve%20zero,time%20inference%20on)). They formulated auxiliary classification/regression tasks where the “label” was a textual description, effectively creating a cross-modal contrastive task between time-series and text. Through this alignment, NormWear achieved **zero-shot inference** capabilities: the model could generalize to recognize health conditions or activities that were *described in words* but never explicitly seen in training ([[2412.09758] Toward Foundation Model for Multivariate Wearable Sensing of Physiological Signals](https://ar5iv.org/html/2412.09758v1#:~:text=We%20achieve%20zero,time%20inference%20on)). This is analogous to vision-language models like CLIP, but for biosignals. For example, one could imagine feeding a description like “the wearer is experiencing high stress” and the model identifying which unlabeled signal segments correspond to that condition. While NormWear’s alignment was limited by the available labeled descriptions in public datasets, it points toward more **semantically aware foundation models** that bridge sensor data and human-interpretable concepts (important for digital phenotyping and clinical use). 

**PaPaGei’s morphology-aware contrastive learning:** PaPaGei (Pillai *et al.* 2024) introduced a creative dual-objective SSL scheme for PPG signals, combining the earlier mentioned participant-aware loss (*PaPaGei-P*) with a **morphology-aware objective** (*PaPaGei-S*). The morphology-aware loss targets the *fine-grained shape characteristics* of PPG waveforms that relate to underlying physiology. Specifically, before training, PaPaGei computes several diagnostic metrics from each PPG segment: e.g., the **stress Vascular Response Index (sVRI)**, which is the ratio of mean PPG amplitude in post-systolic vs. pre-systolic phases (an indicator of arterial resistance) ([PaPaGei: Open Foundation Models for Optical Physiological Signals](https://arxiv.org/html/2410.20542v1#:~:text=PPG%20Morphology,of%20systolic%20to%20diastolic%20areas)) ([PaPaGei: Open Foundation Models for Optical Physiological Signals](https://arxiv.org/html/2410.20542v1#:~:text=introduce%20a%20morphology%20augmentation%20module,other%2C%20with%20sVRI%20capturing%20amplitude)); the **Inflection Point Area (IPA) ratio** between the systolic and diastolic parts of the pulse (related to the dicrotic notch and arterial stiffness) ([PaPaGei: Open Foundation Models for Optical Physiological Signals](https://arxiv.org/html/2410.20542v1#:~:text=key%20PPG%20metrics%20,other%2C%20with%20sVRI%20capturing%20amplitude)); and a signal quality index. These continuous metrics are then **discretized into bins**, and segments falling into the same bin (e.g. PPGs that have a similar sVRI value) are treated as positive pairs ([PaPaGei: Open Foundation Models for Optical Physiological Signals](https://arxiv.org/html/2410.20542v1#:~:text=Training,not%20defined%20based%20on%20subjects)). In other words, PaPaGei defines an entirely new contrastive task: *PPG signals with similar physiological morphology should have similar embeddings*, even if they come from different people. Unlike participant-aware pairing, this ignores subject identity and focuses on cross-subject commonalities in the waveform shape. During training, PaPaGei’s model actually optimizes **three losses in parallel** – likely a contrastive loss for each of sVRI, IPA, and possibly a combined objective ([PaPaGei: Open Foundation Models for Optical Physiological Signals](https://arxiv.org/html/2410.20542v1#:~:text=Training,not%20defined%20based%20on%20subjects)) ([PaPaGei: Open Foundation Models for Optical Physiological Signals](https://arxiv.org/html/2410.20542v1#:~:text=Given%20a%20batch%20of%20PPG,Next%2C%20we)). By doing so, it learns a representation that encodes medically relevant features (like indicators of vascular resistance) without explicit supervision for those features. The outcome was impressive: PaPaGei achieved an average 6.3% improvement in classification performance (and 2.9% in regression) across 20 diverse tasks, compared to other time-series foundation models, and even outperformed models **70× larger** on many tasks ([PaPaGei: Open Foundation Models for Optical Physiological Signals](https://arxiv.org/html/2410.20542v1#:~:text=architecture%20incorporates%20novel%20representation%20learning,Notably)) ([PaPaGei: Open Foundation Models for Optical Physiological Signals](https://arxiv.org/html/2410.20542v1#:~:text=of%206.3,and%20code%20are%20available%20at)). This indicates that targeting domain-specific structure (here, waveform morphology) can yield *more informative representations* than a generic contrastive approach. Additionally, PaPaGei tested the model’s robustness to skin tone variations in PPG and established a benchmark for fairness in such models ([PaPaGei: Open Foundation Models for Optical Physiological Signals](https://arxiv.org/html/2410.20542v1#:~:text=richer%20representations%20than%20traditional%20contrastive,new%20opportunities%20for%20multimodal%20health)) ([PaPaGei: Open Foundation Models for Optical Physiological Signals](https://arxiv.org/html/2410.20542v1#:~:text=of%206.3,and%20code%20are%20available%20at)) – a valuable contribution given concerns about PPG accuracy disparities in different populations.

**Cross-modal knowledge distillation:** Another innovative approach is using one modality to teach another via unlabeled data. Apple’s recent work on **accelerometer foundation models** achieved this through *knowledge distillation* from PPG to accelerometry ([Wearable Accelerometer Foundation Models for Health via Knowledge Distillation - Apple Machine Learning Research](https://machinelearning.apple.com/research/wearable-accelerometer-foundation-models#:~:text=model%20can%20predict%20a%20wide,49%25%20improved)). Accelerometers are ubiquitous and low-power sensors, but not traditionally thought to carry rich health information beyond activity. The Apple team posited that some health signals (e.g. heart rate changes) manifest in subtle motion (chest movement, etc.), and thus an accelerometer might infer them if trained appropriately ([Wearable Accelerometer Foundation Models for Health via Knowledge Distillation - Apple Machine Learning Research](https://machinelearning.apple.com/research/wearable-accelerometer-foundation-models#:~:text=as%20photoplethysmogram%20,minutes%20of%20unlabeled%20data%2C%20collected)) ([Wearable Accelerometer Foundation Models for Health via Knowledge Distillation - Apple Machine Learning Research](https://machinelearning.apple.com/research/wearable-accelerometer-foundation-models#:~:text=widely%20used%20for%20activity%20recognition,We)). They took a large dataset of simultaneous PPG and accelerometer recordings (20 million minutes from ~172k users) and pretrained an accelerometer encoder network to **mimic the representations of a pretrained PPG encoder** ([Wearable Accelerometer Foundation Models for Health via Knowledge Distillation - Apple Machine Learning Research](https://machinelearning.apple.com/research/wearable-accelerometer-foundation-models#:~:text=model%20can%20predict%20a%20wide,49%25%20improved)). Concretely, they likely passed each accelerometer segment through the “student” network and the corresponding PPG segment through the “teacher” (the previously trained PPG foundation model), and minimized a distance between the two embeddings. They report a *99.2% top-1 accuracy* in retrieving the correct PPG embedding given an accelerometer embedding on unseen data ([Wearable Accelerometer Foundation Models for Health via Knowledge Distillation - Apple Machine Learning Research](https://machinelearning.apple.com/research/wearable-accelerometer-foundation-models#:~:text=accelerometery%20encoders%20using%2020%20million,of%20a%20wide%20array%20of)) – effectively, the accelerometer learned to “fake” a PPG-style representation of the same physiological state. This cross-modal alignment is a form of contrastive learning (positive pair = accel vs PPG from same time). The distilled accelerometer model was then evaluated on health prediction tasks. Remarkably, it showed **23–49% improved performance** in predicting heart rate and heart rate variability compared to an accelerometer model trained from scratch, and it was **predictive of a wide array of health targets** (including fitness and wellness metrics) ([Wearable Accelerometer Foundation Models for Health via Knowledge Distillation - Apple Machine Learning Research](https://machinelearning.apple.com/research/wearable-accelerometer-foundation-models#:~:text=consent.%20We%20observe%20strong%20cross,health%20may%20unlock%20new%20opportunities)). In essence, the normally low-fidelity accelerometer became a *generalist health sensor* through knowledge distillation ([Wearable Accelerometer Foundation Models for Health via Knowledge Distillation - Apple Machine Learning Research](https://machinelearning.apple.com/research/wearable-accelerometer-foundation-models#:~:text=show%20that%20distilled%20accelerometry%20encoders,health%20may%20unlock%20new%20opportunities)). This approach opens the door to using foundation models even on sensors that individually have limited information, by *transferring knowledge* from a richer sensor. It also hints at more **modality-to-modality teaching**: for example, could we distill an ECG model into a PPG model or vice-versa, to get the best of both?

**Other notable pretraining tricks:** Some works include **multitask learning** during pretraining. For example, a model might simultaneously solve a contrastive task and a prediction task (multi-objective loss), or autoencode one part of the signal while classifying another. If large labeled datasets exist for a simple related task (e.g., activity labels from wearables), a model might incorporate a supervised loss as one component of pretraining, effectively guiding the representation toward known useful features. However, most “foundation” efforts try to remain as label-agnostic as possible. Another idea is **clustering or pseudo-labeling**: run a preliminary pass to cluster the unlabeled data, assign cluster IDs as fake labels, and train a classifier to predict those – a form of self-supervision that can capture major modes in the data. While we did not see a flagship wearable foundation paper solely built on clustering, it could be combined with other objectives to refine representations (e.g., encourage the model to form distinct states like “rest, exercise, stress” in an unsupervised way).

In summary, the hybrid approaches underscore that **no single pretraining task may be sufficient** to capture all information in complex physiological signals. By combining multiple objectives, researchers aim to learn representations that are *simultaneously invariant and sensitive* to different factors. Participant-aware losses preserve person-specific traits; morphology-aware losses capture domain-specific features; cross-modal alignment brings in external knowledge; semantic alignment connects data to language. These enrich the foundation models beyond what a one-dimensional task could do. As we’ll discuss later, an important future direction is to increase the **diversity of pretraining tasks** even further ([[2412.09758] Toward Foundation Model for Multivariate Wearable Sensing of Physiological Signals](https://ar5iv.org/html/2412.09758v1#:~:text=of%20wearable%20sensing,shot)), much like how multitask training benefited NLP models, so that wearable models can extract every bit of signal from the data.

## Downstream Applications and Capabilities  
A hallmark of foundation models is their versatility – the same pretrained model can be fine-tuned or adapted to many different **downstream tasks**. Wearable foundation models have started to demonstrate this flexibility, delivering strong results across a range of health and wellness applications:

**Biomarker Discovery and Health Prediction:** Because these models ingest such large-scale longitudinal data, they often learn latent representations that correlate with clinical or physiological states, even without explicit labels. For instance, Apple’s PPG/ECG foundation model was found to **encode participant demographics and health conditions** in its embedding space ([Large-scale Training of Foundation Models for Wearable Biosignals - Apple Machine Learning Research](https://machinelearning.apple.com/research/large-scale-training#:~:text=Our%20self,ECG%20foundation%20models%20can%20enhance)). This means that if one probes the representation, one might discover axes that correspond to age, sex, or BMI, or detect signals of conditions like arrhythmia – essentially *digital biomarkers*. The Apple team suggested that these embeddings could help develop new biomarkers by reducing reliance on labeled data ([Large-scale Training of Foundation Models for Wearable Biosignals - Apple Machine Learning Research](https://machinelearning.apple.com/research/large-scale-training#:~:text=Our%20self,ECG%20foundation%20models%20can%20enhance)). In practice, one can take the pretrained model and *fine-tune a simple linear classifier* to predict a health trait (say, cardiorespiratory fitness level); if the representation is rich, even a tiny labeled dataset can suffice to achieve high accuracy. Indeed, many foundation models show excellent **sample efficiency**: NormWear, for example, outperformed prior methods on 18 different health-related tasks with minimal supervised training ([Toward Foundation Model for Multivariate Wearable Sensing of Physiological Signals | OpenReview](https://openreview.net/forum?id=XhdckVyXKg#:~:text=For%20a%20holistic%20assessment%2C%20we,based%20health)). In cardiovascular health, foundation models have set new state-of-the-art results on tasks like atrial fibrillation detection from wearables and respiratory rate estimation from PPG. SiamQuality, after fine-tuning, exceeded prior benchmarks by 5% in AF detection (and a striking 75% in RR estimation) ([
            SiamQuality: a ConvNet-based foundation model for photoplethysmography signals - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11334241/#:~:text=hospitalized%20intensive%20care%20patients%2C%20comprised,robust%20backbone%20for%20foundation%20models)), thanks to its robust representation of noisy PPG. By unlocking such gains, foundation models can facilitate **biomarker discovery** – researchers can search the embedding space for patterns associated with early disease, drug response, etc., possibly revealing insights that were obscured in raw data.

**Anomaly and Event Detection:** Wearable data often contain critical but rare events – an atrial fibrillation episode, a fall, an epileptic seizure, or sensor anomalies. Foundation models trained on *normal* patterns can be powerful tools for detecting *abnormal* ones. A simple approach is to use reconstruction error: for example, Google’s LSM, trained to reconstruct typical sensor data, will likely show a high error if the input sequence is anomalous or outside the distribution (say, a sudden spike due to device glitch or an unprecedented physiological event). This is a common technique in anomaly detection: an autoencoder that cannot reconstruct an event is signaling it’s something *unlike* what it has seen before. Additionally, contrastive embeddings can cluster normal vs. abnormal states distinctly. In ECG, a patient’s normal sinus rhythm and an arrhythmia might end up in very different regions of the embedding space, enabling unsupervised anomaly alerts by distance or density estimation. Some studies explicitly evaluate anomaly detection: e.g., Cheng *et al.* (2021) found their contrastive ECG embeddings useful for unsupervised detection of arrhythmias ([Subject-Aware Contrastive Learning for Biosignals - Apple Machine Learning Research](https://machinelearning.apple.com/research/subject-aware-contrastive-learning#:~:text=time,tuning%20with%20supervised%20labels)). Another domain is data quality anomalies – foundation models like SiamQuality were directly aimed at detecting and correcting low-quality signal segments ([
            SiamQuality: a ConvNet-based foundation model for photoplethysmography signals - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11334241/#:~:text=signals,alignment%20of%20PPG%20representations%20more)) ([
            SiamQuality: a ConvNet-based foundation model for photoplethysmography signals - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11334241/#:~:text=concerns%20are%20particularly%20pronounced,with%2036%20million%20PPG%20signal)). By aligning good and bad signals, SiamQuality’s encoder can flag segments that are likely corrupted (their embeddings would otherwise not match any “good” pattern). As these models get deployed on-device, one can imagine them running continuously to monitor for any deviation from a user’s typical patterns and raising alerts for potential health issues (or simply marking data for review).

**Digital Phenotyping and Behavioral Insights:** Digital phenotyping refers to inferring psychological, cognitive, or behavioral characteristics from sensor data. Wearable foundation models are well-suited for this because they capture subtle patterns over long periods. **NormWear** specifically validated its model on mental health datasets – tasks like detecting stress, mood, or depression from wearable signals ([Toward Foundation Model for Multivariate Wearable Sensing of Physiological Signals | OpenReview](https://openreview.net/forum?id=XhdckVyXKg#:~:text=For%20a%20holistic%20assessment%2C%20we,based%20health)). For example, by training on large EEG and physiological datasets, the model could be fine-tuned to predict self-reported stress levels or cognitive load. The advantage of a foundation model here is that it might pick up complex, multivariate signatures (e.g., a combination of elevated heart rate, poor sleep, and reduced physical activity could signal depressive symptoms) that a single-sensor or single-task model might miss. NormWear’s success across *18 applications* in mental health, activity, and disease risk suggests a broad phenotyping capability ([Toward Foundation Model for Multivariate Wearable Sensing of Physiological Signals | OpenReview](https://openreview.net/forum?id=XhdckVyXKg#:~:text=For%20a%20holistic%20assessment%2C%20we,based%20health)). Additionally, the zero-shot aspect is intriguing: if we have a textual description of a phenotype, NormWear might recognize it in the data without direct training ([Toward Foundation Model for Multivariate Wearable Sensing of Physiological Signals | OpenReview](https://openreview.net/forum?id=XhdckVyXKg#:~:text=improvement%20over%20competitive%20baselines%20in,based%20health)). Beyond health, one could use these models for general behavior inference: detecting routines, changes in sleep patterns, or sociability (via physiological proxies). Tech companies are certainly interested – e.g., analyzing wearable and phone data to infer if someone might be developing a certain condition or just needs a rest. Foundation models provide a single, privacy-preserving representation (since raw data can be converted to embeddings) from which many such phenotypes can be derived with appropriate calibration.

**Context and Activity Recognition:** Many classical wearables tasks involve recognizing what the user is doing (sitting, running, sleeping) or their context (at home, at work). Foundation models trained on multimodal signals can achieve excellent performance here, often with *zero-shot or few-shot*. Google’s LSM, for instance, after masked pretraining, was evaluated on exercise and activity recognition tasks and showed superior sample-efficient learning ([[2410.13638] Scaling Wearable Foundation Models](https://ar5iv.org/abs/2410.13638#:~:text=establish%20the%20scaling%20laws%20of,like%20exercise%20and%20activity%20recognition)). A model that has seen 165k people’s sensor traces likely knows the difference between walking and sleeping just from patterns of motion, heart rate, and so on. If such a model is embedded in a smartwatch, it could potentially label new activities on the fly or adapt to novel activities by similarity (e.g., identify cycling vs. running by relating to known patterns). Activity recognition is a somewhat solved problem with enough labeled data, but foundation models make it easier to scale to new domains and edge cases – for example, recognizing *micro-activities* or unusual situations (like distinguishing different types of workouts, or detecting when someone is driving versus riding a bus, based purely on motion and vital signs). The foundation model provides a general feature space where these differences are salient, so a developer can quickly train a classifier for a new class with minimal data.

**Personalization:** Interestingly, while foundation models aim to be general, they can also assist personalization. A pretrained model can be fine-tuned on a single user’s data to create a personalized model that still benefits from global knowledge. For example, one could take a foundation model and fine-tune it on a specific patient’s previous ECG recordings to better detect that patient’s arrhythmias – the base features ensure important patterns are recognized, and the fine-tuning accounts for that patient’s idiosyncratic ECG morphology. This approach (sometimes called *few-shot personalization*) is promising for wearables, since each user generates a lot of unlabeled data of their own. Some studies like **Subject-aware learning** inherently think in terms of per-subject adaptation: they showed that a foundation embedding can be adjusted with a little supervised data to significantly improve performance for that specific subject ([Subject-Aware Contrastive Learning for Biosignals - Apple Machine Learning Research](https://machinelearning.apple.com/research/subject-aware-contrastive-learning#:~:text=different%20biosignals%20with%20different%20tasks%3A,tuning%20with%20supervised%20labels)). In health, this could mean personalized baselines and alerts (the model knows *your* normal range and flags deviations accordingly).

Overall, wearable foundation models are demonstrating **extreme flexibility**: one model can underpin applications ranging from medical diagnosis support to wellness coaching. By training on broad data, these models embody a kind of general knowledge about human physiology in everyday life. This can accelerate development of new digital health solutions because one no longer needs to start from scratch for each task. Instead, one can leverage a pretrained model (open-sourced ones like PaPaGei ([PaPaGei: Open Foundation Models for Optical Physiological Signals](https://arxiv.org/html/2410.20542v1#:~:text=for%20other%20multimodal%20models%2C%20opening,and%20code%20are%20available%20at)) are available) and fine-tune it for a specific dataset or integrate it into an app that monitors certain metrics. The result is better performance with less data and often the ability to tackle tasks that were previously impractical due to data scarcity (e.g., predicting long-term health outcomes from subtle signals).

## Limitations and Future Opportunities  

Despite rapid progress, wearable foundation models are still in their early days, and **several gaps and challenges** remain. Here we discuss some key limitations and promising directions to overcome them, particularly focusing on making these models more generalizable, efficient, and privacy-conscious:

**1. Generalizability vs. Personalization:** There is an inherent tension between training a general model that works for everyone and recognizing the uniqueness of each individual’s data. Many current models strive for generalization across heterogeneous datasets ([Toward Foundation Model for Multivariate Wearable Sensing of Physiological Signals | OpenReview](https://openreview.net/forum?id=XhdckVyXKg#:~:text=For%20a%20holistic%20assessment%2C%20we,based%20health)), but **inter-subject variability** can still hinder performance ([Subject-Aware Contrastive Learning for Biosignals - Apple Machine Learning Research](https://machinelearning.apple.com/research/subject-aware-contrastive-learning#:~:text=approach%20based%20on%20contrastive%20learning,EEG%20decoding%20and%20ECG%20anomaly)). For instance, a foundation model might still carry a “bias” towards the majority of training subjects, underperforming for those with rarer physiology or conditions. On the other hand, models that encode subject-specific features (like Apple’s participant-level contrastive approach) capture personal nuances but might **overfit to individuals**, reducing transferability. A future opportunity is to develop techniques for **efficient personalization on top of a general foundation**. This could include fine-tuning a small portion of the model (or using adapters) on a new user’s data to specialize it, or meta-learning approaches that allow quick adaptation. Importantly, this can be done **on-device** to preserve privacy (e.g., a user’s watch fine-tunes the model to their data locally). Such personalization would yield a hybrid: the model has seen “everyone” but is also specially calibrated to “you.” Early research in this vein uses federated learning or on-device training to continuously adapt models while sharing knowledge across users in a privacy-preserving way. Balancing general patterns and personal baselines is crucial for sensitive health applications where one size may not fit all.

**2. Data Diversity and Bias:** Foundation models are only as good as the data they are trained on. While recent models use much larger datasets than before, there are still concerns about **coverage**. Certain demographic groups or clinical conditions might be underrepresented. For example, if a PPG foundation model is trained mostly on young, healthy individuals, its performance might degrade in older patients or those with vascular disease. Similarly, devices and settings matter – data from one brand of smartwatch or from clinical monitors might not translate perfectly to another. Researchers have begun to address bias (e.g., PaPaGei evaluated skin tone effects on PPG readings ([PaPaGei: Open Foundation Models for Optical Physiological Signals](https://arxiv.org/html/2410.20542v1#:~:text=of%206.3,and%20code%20are%20available%20at)), since darker skin can alter PPG signals). Future foundation models should explicitly incorporate strategies to mitigate bias: ensuring training data spans ages, ethnicities, fitness levels, etc., and maybe using **reweighting or adversarial training** to prevent the model from latching onto spurious correlations (like motion noise that correlates with gender, for instance). Additionally, models should be tested for **out-of-distribution generalization** – can a model trained on mostly American adults also perform well on, say, Asian pediatric data? Initiatives to create larger, more diverse open datasets for wearables would greatly aid this. Moreover, foundation models can be used to **generate synthetic data** to fill gaps (some, like Chronos, even trained on synthetic GP data ([[2403.07815] Chronos: Learning the Language of Time Series](https://arxiv.org/abs/2403.07815#:~:text=via%20the%20cross,Our%20results))). Using generative models to augment underrepresented scenarios (e.g., rare arrhythmia events, or data from minority groups) might help improve robustness.

**3. Efficiency and On-Device Deployment:** Many current foundation models are large and computationally heavy (e.g., hundreds of millions of parameters) ([[2403.07815] Chronos: Learning the Language of Time Series](https://arxiv.org/abs/2403.07815#:~:text=,classical%20local%20models%20and%20deep)). This is fine for cloud computing but problematic for wearable devices which have limited battery and processing power. A major goal is to create **efficient foundation models** that can actually run on-device for real-time inference. Some works have made progress: SiamQuality deliberately used a CNN backbone instead of transformers to keep it lightweight ([
            SiamQuality: a ConvNet-based foundation model for photoplethysmography signals - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11334241/#:~:text=In%20this%20study%2C%20we%20propose,from%20hospitalized%20intensive%20care%20patients)). Knowledge distillation, as applied to accelerometers, is another way to compress knowledge into a smaller model ([Wearable Accelerometer Foundation Models for Health via Knowledge Distillation - Apple Machine Learning Research](https://machinelearning.apple.com/research/wearable-accelerometer-foundation-models#:~:text=model%20can%20predict%20a%20wide,49%25%20improved)) ([Wearable Accelerometer Foundation Models for Health via Knowledge Distillation - Apple Machine Learning Research](https://machinelearning.apple.com/research/wearable-accelerometer-foundation-models#:~:text=consent.%20We%20observe%20strong%20cross,health%20may%20unlock%20new%20opportunities)). Techniques like model pruning, quantization, and *TinyML* optimizations are natural next steps – for instance, taking a large pretrained model and distilling it into a smaller one that approximates the large model’s outputs. There is also interest in **transformer alternatives** for time-series that are more efficient (such as structured state-space models or low-rank transformers) that could be used as foundation model architectures. Another angle is *on-device pretraining*: can a watch or phone incrementally train a model on its user’s data during idle times? This is challenging due to resource limits, but even partial adaptation could be feasible. Ultimately, bridging the gap between giant “server-side” foundation models and deployable “edge” models will be key to actually realizing the benefits in consumer devices. A promising notion is a **two-tier model**: a smaller on-device model that was distilled from a larger foundation model, capable of running in real-time and perhaps periodically syncing with a cloud model for updates (while keeping raw data private).

**4. Privacy-Preserving Learning:** Health data from wearables is highly personal, and users are rightly concerned about privacy. Current foundation models, especially those from industry, rely on centralizing large datasets (albeit de-identified) for training ([[2410.13638] Scaling Wearable Foundation Models](https://ar5iv.org/abs/2410.13638#:~:text=we%20investigate%20the%20scaling%20properties,downstream%20learning%20for%20tasks%20like)). In the future, methods like **Federated Learning (FL)** could enable training foundation models **without collecting raw data** centrally. In a federated scenario, each device (or each cohort at different hospitals) would compute model updates on local data, and only those updates (not the raw signals) are sent to a central server to be aggregated into a global model. This has been explored in other domains and is very relevant for wearables, as companies like Google and Apple have already deployed FL for some fitness and keyboard prediction models. Applying FL to the self-supervised pretraining phase is an open research area – challenges include communication efficiency (SSL can be heavy), handling unbalanced data (some users generate much more data), and ensuring convergence of very large models in a decentralized way. Another privacy tool is **differential privacy**, which could be used to noise or clip contributions from any single user such that the final model cannot leak individual information. While these techniques might degrade model quality slightly, the trade-off could be worth it if it enables training on sensitive medical data that otherwise would not be shared. In sum, developing **privacy-preserving foundation models** is crucial for trust and widespread adoption in healthcare. Users may be more willing to contribute their data to improving models if they know it never leaves their device or is rigorously protected.

**5. Multimodal and Contextual Modeling:** So far, most wearable foundation models focus on a set of **physiological signals** from on-body sensors. But there is a rich context around those signals – e.g., smartphone data (GPS location, phone usage), ambient data (weather, air quality), even textual data like user diaries or medical records. Integrating these modalities could vastly enhance what foundation models can do (as also evidenced by trends in general AI toward multimodal models). For example, a truly holistic health foundation model might take in not just PPG and accelerometer, but also the user’s calendar (to know workdays vs weekends), their diet log, and their genome! While that grand integration is beyond current scope, steps in that direction are happening. Apple’s multimodal experiments on PhysioNet data showed that **cross-modal objectives** (like reconstructing one signal from others) improve performance ([Promoting Cross-Modal Representations to Improve Multimodal Foundation Models for Physiological Signals - Apple Machine Learning Research](https://machinelearning.apple.com/research/modal-representations#:~:text=costly%2C%20there%20is%20a%20lot,We%20also%20find%20that%20late)) ([Promoting Cross-Modal Representations to Improve Multimodal Foundation Models for Physiological Signals - Apple Machine Learning Research](https://machinelearning.apple.com/research/modal-representations#:~:text=successful%20multimodal%20training%2C%20as%20they,health%20data%2C%20even%20across%20diverse)). There’s potential to incorporate **environmental sensors** (a model that knows “pollen count is high” could factor that into stress or allergy predictions) or **user-input data** (like momentary mood reports, which could be treated as another channel or used in alignment as NormWear did with text). Another aspect of context is temporal context at scale: current models typically look at minutes or hours of data at once (Google LSM used 300-minute windows ([[2410.13638] Scaling Wearable Foundation Models](https://ar5iv.org/abs/2410.13638#:~:text=magnitude%20due%20to%20different%20units,leading%20missing%20minutes%20were%20backfilled))). But health patterns sometimes unfold over weeks and months (think menstrual cycles, seasonal changes in activity). Future foundation models might employ hierarchical modeling to capture both short-term and long-term patterns – for instance, a transformer that summarizes daily trends over a year-long sequence. This ties into the *long-context* research in time-series (some work, e.g., from AWS on long-sequence models, is ongoing ([Towards Long-Context Time Series Foundation Models - arXiv](https://arxiv.org/html/2409.13530#:~:text=Towards%20Long,series%20forecasting))). Extending foundation models to incorporate **wider context, additional modalities, and longer horizons** would make them even more powerful tools for digital phenotyping and prediction.

**6. Task Diversity and Evaluation:** As noted by Fang *et al.* (2024), methods for foundation models in health are still in early exploration and it’s **unclear which pretraining strategies are most effective** given the diversity of signals ([Promoting Cross-Modal Representations to Improve Multimodal Foundation Models for Physiological Signals - Apple Machine Learning Research](https://machinelearning.apple.com/research/modal-representations#:~:text=improving%20machine%20learning%20methods%20for,use%20a%20masked%20autoencoding%20objective)). Different works use different proxy tasks, and there hasn’t been a unified benchmark to evaluate them (though efforts like evaluating on 20+ tasks in NormWear ([Toward Foundation Model for Multivariate Wearable Sensing of Physiological Signals | OpenReview](https://openreview.net/forum?id=XhdckVyXKg#:~:text=For%20a%20holistic%20assessment%2C%20we,based%20health)) and PaPaGei ([PaPaGei: Open Foundation Models for Optical Physiological Signals](https://arxiv.org/html/2410.20542v1#:~:text=hours%20of%2020%20million%20unlabeled,series)) are a start). One gap is that most works optimize for a relatively narrow set of pretext tasks – often either contrastive or masked objectives. There is an opportunity to **increase the diversity of pretraining tasks** to force the model to learn more comprehensive features ([[2412.09758] Toward Foundation Model for Multivariate Wearable Sensing of Physiological Signals](https://ar5iv.org/html/2412.09758v1#:~:text=of%20wearable%20sensing,shot)). In NLP, for instance, some models combined language modeling with next sentence prediction, etc. In wearables, one could envision multi-task self-supervision: simultaneously do masked reconstruction, segment reordering, heartbeat waveform masking, clustering, and contrastive across modalities, all in one framework. This would of course complicate training but might yield a single model that understands the data from many angles. The community would benefit from **benchmark datasets and tasks** to compare foundation models – similar to GLUE in NLP or HEAR for audio. Perhaps a “PhysioGLUE” could be created, compiling a suite of downstream evaluations (heart disease classification, activity detection, sleep stage prediction, etc.) to systematically assess how well a foundation model transfers. This would also shine light on what current models are missing. For example, if a model performs poorly on a certain task, it might indicate the need for a new pretraining signal related to that task.

In conclusion, wearable foundation models are an exciting and fast-moving frontier at the intersection of AI and health. The past three years have demonstrated the feasibility and utility of training large models on longitudinal physiological data, yielding **general-purpose representations** that can drive many applications. These models employ a rich toolkit of self-supervised objectives – from **contrastive learning that aligns disparate signals** to **masked modeling that reconstructs them**, and innovative hybrids that incorporate domain knowledge. They have begun to show benefits in improving predictions of health and behavior, often with far fewer labels than previously required. Yet, significant work remains to **realize their full potential**. By addressing current limitations – improving generalization across people, ensuring fairness and diversity, shrinking models for on-device use, protecting privacy, and embracing the full multimodal nature of health – future foundation models could become key enablers of personalized digital medicine. Imagine a not-so-distant future where your smartwatch hosts a powerful yet efficient foundation model that continuously interprets your biosignals, providing early warnings, actionable insights, or just better health metrics, all while keeping your data secure. The research efforts of 2022–2025 have laid the groundwork for this vision by proving that foundation models can indeed be learned from wearable data ([[2410.13638] Scaling Wearable Foundation Models](https://ar5iv.org/abs/2410.13638#:~:text=empirical%20success%20of%20generative%20modeling%2C,tasks%20such%20as%20imputation%2C%20interpolation)) ([Large-scale Training of Foundation Models for Wearable Biosignals](https://arxiv.org/html/2312.05409v2#:~:text=supervised%20learning%20using%20the%20unlabeled,trained%20foundation%20models%20readily%20encode)). The coming years will undoubtedly build on this foundation, making our wearables smarter and our health insights deeper than ever before.

**Sources:** Recent research and review literature on foundation models for wearables and time-series, including Google’s Large Sensor Model ([[2410.13638] Scaling Wearable Foundation Models](https://ar5iv.org/abs/2410.13638#:~:text=we%20investigate%20the%20scaling%20properties,downstream%20learning%20for%20tasks%20like)) ([[2410.13638] Scaling Wearable Foundation Models](https://ar5iv.org/abs/2410.13638#:~:text=optimize%20the%20masked%20signal%20reconstruction,across%20all%20the%20normalized%20signals)), Apple’s wearable biosignal models ([Large-scale Training of Foundation Models for Wearable Biosignals - Apple Machine Learning Research](https://machinelearning.apple.com/research/large-scale-training#:~:text=Our%20self,ECG%20foundation%20models%20can%20enhance)) ([Wearable Accelerometer Foundation Models for Health via Knowledge Distillation - Apple Machine Learning Research](https://machinelearning.apple.com/research/wearable-accelerometer-foundation-models#:~:text=model%20can%20predict%20a%20wide,49%25%20improved)), Nokia Bell Labs’ PaPaGei PPG model ([PaPaGei: Open Foundation Models for Optical Physiological Signals](https://arxiv.org/html/2410.20542v1#:~:text=PPG%20Morphology,of%20systolic%20to%20diastolic%20areas)) ([PaPaGei: Open Foundation Models for Optical Physiological Signals](https://arxiv.org/html/2410.20542v1#:~:text=Training,not%20defined%20based%20on%20subjects)), the NormWear framework ([Toward Foundation Model for Multivariate Wearable Sensing of Physiological Signals | OpenReview](https://openreview.net/forum?id=XhdckVyXKg#:~:text=For%20a%20holistic%20assessment%2C%20we,based%20health)) ([[2412.09758] Toward Foundation Model for Multivariate Wearable Sensing of Physiological Signals](https://ar5iv.org/html/2412.09758v1#:~:text=Image%3A%20Refer%20to%20caption%20Figure,Overview%20of%20the%20pretrain%20pipeline)), and surveys of time-series foundation techniques ([[2403.14735] Foundation Models for Time Series Analysis: A Tutorial and Survey](https://ar5iv.org/html/2403.14735v3#:~:text=domains.%20Echoing%20Lag,samples%20of%20the%20target%20distribution)) ([Promoting Cross-Modal Representations to Improve Multimodal Foundation Models for Physiological Signals - Apple Machine Learning Research](https://machinelearning.apple.com/research/modal-representations#:~:text=costly%2C%20there%20is%20a%20lot,We%20also%20find%20that%20late)), among others. These works span top venues (NeurIPS, ICLR, KDD) and industry research reports from 2023–2025. Each demonstrates different self-supervised strategies and downstream evaluations, which collectively shape the landscape of wearable foundation models outlined in this review.
